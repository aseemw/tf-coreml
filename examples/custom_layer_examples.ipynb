{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the process of adding custom layers to the CoreML model during conversion.  \n",
    "CoreML supports a certain set of Neural Network layers. The list can be found [here](https://github.com/apple/coremltools/blob/master/mlmodel/format/NeuralNetwork.proto) or [here](https://apple.github.io/coremltools/coremlspecification/sections/NeuralNetwork.html).\n",
    "For TensorFlow operations (ops for short) that are not translatable to any of these CoreML layers, custom layers can be inserted in the CoreML model. At runtime, CoreML framework will look for the implementation code of the custom layer, which has to be provided by the developer in her app.   \n",
    "Custom layer is a [proto message](https://github.com/apple/coremltools/blob/5b5b8190764ffe78110be6b4d0edbeebe0253a6e/mlmodel/format/NeuralNetwork.proto#L2280), like any other neural netowrk layer in the .mlmodel file (which is in protobuf format), that can hold the parameters and weights (if any) associated with the TF op.\n",
    "Here is more [documentation](https://developer.apple.com/documentation/coreml/core_ml_api/creating_a_custom_layer) on CoreML custom layers. \n",
    "\n",
    "\n",
    "Lets start by defining a toy TF graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aseem/Aseem_env/env/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "WARNING:root:Keras version 2.1.5 detected. Last version known to be fully compatible of Keras is 2.1.3 .\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.python.tools.freeze_graph import freeze_graph\n",
    "import numpy as np\n",
    "import shutil\n",
    "import tempfile\n",
    "import os\n",
    "import tfcoreml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF out:  UnitNorm/div (1, 10) 0.99999654\n"
     ]
    }
   ],
   "source": [
    "model_dir = tempfile.mkdtemp()\n",
    "graph_def_file = os.path.join(model_dir, 'tf_graph.pbtxt')\n",
    "checkpoint_file = os.path.join(model_dir, 'tf_model.ckpt')\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default() as g:\n",
    "    inputs = tf.placeholder(tf.float32, shape=[None,8], name='input')\n",
    "    with slim.arg_scope([slim.fully_connected],\n",
    "          weights_initializer=tf.truncated_normal_initializer(0.0, 0.2),\n",
    "          weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "        y = slim.fully_connected(inputs, 10, scope='fc')\n",
    "        y = slim.unit_norm(y,dim=1)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "output_name = y.op.name\n",
    "X = np.random.rand(1,8)\n",
    "\n",
    "with tf.Session(graph = graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    out = sess.run(y,feed_dict={inputs: X}) \n",
    "    tf.train.write_graph(sess.graph, model_dir, graph_def_file)\n",
    "    saver.save(sess, checkpoint_file)\n",
    "    \n",
    "\n",
    "print 'TF out: ', output_name, out.shape, np.sum(out ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/09/hn9tml7d58nggsyxt54ymw5h0000gp/T/tmpig1YdI/tf_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/09/hn9tml7d58nggsyxt54ymw5h0000gp/T/tmpig1YdI/tf_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 2 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 2 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 2 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "# generate the frozen .pb file\n",
    "def _simple_freeze(input_graph, input_checkpoint, output_graph, output_node_names):\n",
    "    freeze_graph(input_graph=input_graph,\n",
    "                 input_saver=\"\",\n",
    "                 input_binary=False,\n",
    "                 input_checkpoint=input_checkpoint,\n",
    "                 output_node_names=output_node_names,\n",
    "                 restore_op_name=\"save/restore_all\",\n",
    "                 filename_tensor_name=\"save/Const:0\",\n",
    "                 output_graph=output_graph,\n",
    "                 clear_devices=True,\n",
    "                 initializer_nodes=\"\")\n",
    "    \n",
    "\n",
    "frozen_model_file = 'unit_norm_graph.pb'\n",
    "_simple_freeze(input_graph=graph_def_file,\n",
    "                input_checkpoint=checkpoint_file,\n",
    "                output_graph=frozen_model_file,\n",
    "                output_node_names=output_name)\n",
    "      \n",
    "if os.path.exists(model_dir):\n",
    "    shutil.rmtree(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import netron\n",
    "netron.serve_file(frozen_model_file, browse = True, port=8090)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try to convert it \n",
    "coreml_model = tfcoreml.convert(\n",
    "        tf_model_path=frozen_model_file,\n",
    "        mlmodel_path='unit_norm_graph.mlmodel',\n",
    "        input_name_shape_dict={'input:0':[1,8]},\n",
    "        output_feature_names=[output_name+':0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes not found for 10 tensors. Executing graph to determine shapes. \n",
      "1/26: Analysing op name: UnitNorm/concat/axis ( type:  Const )\n",
      "2/26: Analysing op name: UnitNorm/StridedSlice/end ( type:  Const )\n",
      "3/26: Analysing op name: UnitNorm/StridedSlice/begin ( type:  Const )\n",
      "4/26: Analysing op name: UnitNorm/ones_like/Const ( type:  Const )\n",
      "5/26: Analysing op name: UnitNorm/ones_like/Shape ( type:  Const )\n",
      "6/26: Analysing op name: UnitNorm/ones_like ( type:  Fill )\n",
      "7/26: Analysing op name: UnitNorm/ones ( type:  Const )\n",
      "8/26: Analysing op name: UnitNorm/add/x ( type:  Const )\n",
      "9/26: Analysing op name: UnitNorm/Sum/reduction_indices ( type:  Const )\n",
      "10/26: Analysing op name: fc/biases ( type:  Const )\n",
      "11/26: Analysing op name: fc/biases/read ( type:  Identity )\n",
      "12/26: Analysing op name: fc/weights ( type:  Const )\n",
      "13/26: Analysing op name: fc/weights/read ( type:  Identity )\n",
      "14/26: Analysing op name: input ( type:  Placeholder )\n",
      "Skipping name of placeholder\n",
      "15/26: Analysing op name: fc/MatMul ( type:  MatMul )\n",
      "16/26: Analysing op name: fc/BiasAdd ( type:  BiasAdd )\n",
      "17/26: Analysing op name: fc/Relu ( type:  Relu )\n",
      "18/26: Analysing op name: UnitNorm/Shape ( type:  Shape )\n",
      "19/26: Analysing op name: UnitNorm/StridedSlice ( type:  StridedSlice )\n",
      "20/26: Analysing op name: UnitNorm/concat ( type:  ConcatV2 )\n",
      "21/26: Analysing op name: UnitNorm/Square ( type:  Square )\n",
      "22/26: Analysing op name: UnitNorm/Sum ( type:  Sum )\n",
      "23/26: Analysing op name: UnitNorm/add ( type:  Add )\n",
      "24/26: Analysing op name: UnitNorm/Sqrt ( type:  Sqrt )\n",
      "25/26: Analysing op name: UnitNorm/Tile ( type:  Tile )\n",
      "Adding custom layer\n",
      "26/26: Analysing op name: UnitNorm/div ( type:  RealDiv )\n",
      "\n",
      " Core ML model generated. Saved at location: unit_norm_graph.mlmodel \n",
      "\n",
      "Core ML input(s): \n",
      " [name: \"input__0\"\n",
      "type {\n",
      "  multiArrayType {\n",
      "    shape: 8\n",
      "    dataType: DOUBLE\n",
      "  }\n",
      "}\n",
      "]\n",
      "Core ML output(s): \n",
      " [name: \"UnitNorm__div__0\"\n",
      "type {\n",
      "  multiArrayType {\n",
      "    shape: 10\n",
      "    dataType: DOUBLE\n",
      "  }\n",
      "}\n",
      "]\n",
      "\n",
      "\n",
      "Custom layers have been added to the CoreML model corresponding to the following ops in the TF graph: \n",
      "1/1: op type: Tile, op input names and shapes: [('UnitNorm/Sqrt:0', [1, 1]), ('UnitNorm/concat:0', [2])], op output names and shapes: [('UnitNorm/Tile:0', [1, 10])]\n"
     ]
    }
   ],
   "source": [
    "# we get an unsupported op error. Try again with custom Flag set to true\n",
    "coreml_model = tfcoreml.convert(\n",
    "        tf_model_path=frozen_model_file,\n",
    "        mlmodel_path='unit_norm_graph.mlmodel',\n",
    "        input_name_shape_dict={'input:0':[1,8]},\n",
    "        output_feature_names=['UnitNorm/div:0'],\n",
    "        add_custom_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 'unit_norm_graph.mlmodel'\n",
      "Serving 'unit_norm_graph.mlmodel' at http://localhost:8091\n",
      "\n",
      "Stopping\n"
     ]
    }
   ],
   "source": [
    "# visualize CoreML model\n",
    "import netron\n",
    "netron.serve_file('unit_norm_graph.mlmodel', browse = True, port=8091)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: layer type: (innerProduct) , inputs: input__0, outputs: fc/BiasAdd:0\n",
      "1: layer type: (activation) , inputs: fc/BiasAdd:0, outputs: fc/Relu:0\n",
      "2: layer type: (loadConstant) , inputs: , outputs: UnitNorm/StridedSlice:0\n",
      "3: layer type: (loadConstant) , inputs: , outputs: UnitNorm/ones:0\n",
      "4: layer type: (concat) , inputs: UnitNorm/ones:0, UnitNorm/StridedSlice:0, outputs: UnitNorm/concat:0\n",
      "5: layer type: (multiply) , inputs: fc/Relu:0, fc/Relu:0, outputs: UnitNorm/Square:0\n",
      "6: layer type: (reduce) , inputs: UnitNorm/Square:0, outputs: UnitNorm/Sum:0\n",
      "7: layer type: (add) , inputs: UnitNorm/Sum:0, outputs: UnitNorm/add:0\n",
      "8: layer type: (unary) , inputs: UnitNorm/add:0, outputs: UnitNorm/Sqrt:0\n",
      "9: layer type: (custom) , inputs: UnitNorm/Sqrt:0, UnitNorm/concat:0, outputs: UnitNorm/Tile:0\n",
      "10: layer type: (unary) , inputs: UnitNorm/Tile:0, outputs: inversed_UnitNorm/Tile:0_UnitNorm/div:0\n",
      "11: layer type: (multiply) , inputs: fc/Relu:0, inversed_UnitNorm/Tile:0_UnitNorm/div:0, outputs: UnitNorm__div__0\n"
     ]
    }
   ],
   "source": [
    "# inspect the CoreML model\n",
    "import coremltools\n",
    "spec = coreml_model.get_spec()\n",
    "nn_layers = coremltools.models.utils._get_nn_layers(spec)\n",
    "for i, layer in enumerate(nn_layers):\n",
    "    print('{}: layer type: ({}) , inputs: {}, outputs: {}'.\n",
    "          format(i,layer.WhichOneof('layer'), \", \".join([x for x in layer.input]), \", \".join([x for x in layer.output])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"UnitNorm__Tile\"\n",
      "input: \"UnitNorm/Sqrt:0\"\n",
      "input: \"UnitNorm/concat:0\"\n",
      "output: \"UnitNorm/Tile:0\"\n",
      "custom {\n",
      "  className: \"Tile\"\n",
      "  description: \"Custom layer that corresponds to the TensorFlow op Tile\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we see that the 10-th layer is the custom layer. Lets print it fully.\n",
    "print(nn_layers[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
